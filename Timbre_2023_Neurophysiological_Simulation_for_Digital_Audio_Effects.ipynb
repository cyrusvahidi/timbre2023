{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrusvahidi/timbre2023/blob/main/Timbre_2023_Neurophysiological_Simulation_for_Digital_Audio_Effects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x0pus7DwQk4"
      },
      "source": [
        "# Timbre 2023: Neurophysiological Simulation for Digital Audio Effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxUUYmglq1cx"
      },
      "source": [
        "Welcome to Neurophysiological Simulation for Digital Audio Effects at Timbre 2023\n",
        "\n",
        "This is a tutorial about scattering transforms in Kymatio.\n",
        "\n",
        "\n",
        "## **What is Kymatio?**\n",
        "\n",
        "[Kymatio](https://kymat.io) [(Andreux et al. 2020)](https://scholar.google.com/scholar_url?url=https://www.jmlr.org/papers/volume21/19-047/19-047.pdf&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=0&d=13496502122111863300&ei=gZamZKPcB9KsmgHR5qyoDg&scisig=ABFrs3xtOuMXLkkilckRcYmjWbZA) is an open-source Python package for applications at the intersection of deep learning and wavelet scattering.\n",
        "\n",
        "Its forthcoming stable release (v0.4) provides an implementation of the joint time—frequency scattering transform (JTFS).\n",
        "\n",
        "\n",
        "## **Why scattering at Timbre 2023?**\n",
        "\n",
        "JTFS is an idealisation of a neurophysiological model that is commonly known in timbre perception research: the spectrotemporal receptive field (STRF) [Patil et al., 2012](https://scholar.google.com/scholar_url?url=https://journals.plos.org/ploscompbiol/article%3Fid%3D10.1371/journal.pcbi.1002759&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=0&d=13643030775703850815&ei=4JamZO6rEd2Sy9YP2fyhwAg&scisig=ABFrs3xGck11FtfqtyU_-VGjR1TE).\n",
        "\n",
        "The STRF simulates processing in the mammalian primary cortex, based on empirical measurements of the response of neurons to moving ripple spectra.\n",
        "\n",
        "JTFS offers benefits from the STRF in five factors: (i) differentiability, (ii)  efficiency, (iii) numerical accuracy, (iv) GPU-compatibility and (v) portability across scientific computing and machine learning frameworks in Python.\n",
        "\n",
        "Of relevance to timbre research, we previously demonstrated that JTFS accurately represents similarity between spectrotemporal modulations and serves as a state-of-the-art feature extractor for musical instrument classification (Muradeli et al., 2022). Spectrotemporal modulations in the STRF have shown to serve as a predictor of timbre dissimilarity judgements (Patil et al., 2012, Thoret et al., 2021).\n",
        "\n",
        "Likewise, Euclidean distances in JTFS can predict human judgments of similarity between musical instrument playing techniques (Lostanlen et al., 2021).\n",
        "\n",
        "Recently, JTFS has been used as a loss function to control sound synthesis parameters that generate spectrotemporal modulations (Vahidi et al., 2023).\n",
        "\n",
        "\n",
        "## **Scattering tools in this notebook**:\n",
        "\n",
        "In this notebook, we will gain intuition on:\n",
        "\n",
        "- Understanding Kymatio's frontend parameters and how the scattering transform works\n",
        "- How the scattering transform represents the physical properties of modulated signals\n",
        "- The scattering transform's time and frequency transposition invariance\n",
        "- Acoustic modelling of sound synthesis parameters\n",
        "\n",
        "We will tour Kymatio for audio signals, via:\n",
        "\n",
        "- Filterbank construction and visualization\n",
        "- `Scattering1D` and `TimeFrequencyScattering` frontends\n",
        "- Visualizing first-order ($ S_1 x [\\lambda, t]$) and second-order ($S_2 x [\\lambda, \\lambda_2, t]$) scattering coefficients\n",
        "- Similarity retrieval of spectrotemporal modulation signals\n",
        "- Differentiable modelling of spectrotemporal modulations\n",
        "- Comparison to short-time Fourier representation of music signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUTCZn5QwueX"
      },
      "source": [
        "## **Authors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hgwoIV8wvY0"
      },
      "source": [
        "Cyrus Vahidi (https://github.com/cyrusvahidi/, https://twitter.com/cyrusasfa)\n",
        "\n",
        "Vincent Lostanlen (https://github.com/lostanlen/, https://twitter.com/lostanlen)\n",
        "\n",
        "Kymatio (https://github.com/kymatio/kymatio)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k__FszzjwmNn"
      },
      "source": [
        "## **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACYVqal2wn6K"
      },
      "source": [
        "- [Andreux, M. et al. (2020). **Kymatio: Scattering transforms in Python.** The Journal of Machine Learning Research, 21(1), 2256-2261.](https://scholar.google.com/scholar_url?url=https://www.jmlr.org/papers/volume21/19-047/19-047.pdf&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=0&d=13496502122111863300&ei=gZamZKPcB9KsmgHR5qyoDg&scisig=ABFrs3xtOuMXLkkilckRcYmjWbZA)\n",
        "- [Lostanlen, V., El-Hajj, C., Rossignol, M., Lafay, G., Andén, J., & Lagrange, M. (2021). **Time–frequency scattering accurately models auditory similarities between instrumental playing techniques.** EURASIP Journal on Audio, Speech, and Music Processing, 2021(1), 1-21.](https://scholar.google.com/scholar_url?url=https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00187-z&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=0&d=14039807909065615562&ei=vpamZJDaNcr2mgH68LmoAg&scisig=ABFrs3yvkUpho3swu_2K3TZKa7LO)\n",
        "- [Muradeli, J., Vahidi, C., Wang, C., Han, H., Lostanlen, V., Lagrange, M., & Fazekas, G. (2022, September). **Differentiable Time-Frequency Scattering On GPU. In Digital Audio Effects Conference (DAFx).**](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2204.08269&hl=en&sa=T&oi=gsb&ct=res&cd=1&d=13536706587774301644&ei=ypamZL3ZHPqDy9YP-7iN-Ak&scisig=ABFrs3zg-sPJPLN_fVnXOZax2p31)\n",
        "- [Patil, K., Pressnitzer, D., Shamma, S., & Elhilali, M. (2012). **Music in our ears: the biological bases of musical timbre perception.** PLoS computational biology, 8(11), e1002759.](https://scholar.google.com/scholar_url?url=https://journals.plos.org/ploscompbiol/article%3Fid%3D10.1371/journal.pcbi.1002759&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=0&d=13643030775703850815&ei=4JamZO6rEd2Sy9YP2fyhwAg&scisig=ABFrs3xGck11FtfqtyU_-VGjR1TE)\n",
        "- [Thoret, E., Caramiaux, B., Depalle, P., & Mcadams, S. (2021). **Learning metrics on spectrotemporal modulations reveals the perception of musical instrument timbre.** Nature Human Behaviour, 5(3), 369-377.](https://scholar.google.com/scholar_url?url=https://www.nature.com/articles/s41562-020-00987-5&hl=en&sa=T&oi=gsb&ct=res&cd=0&d=2045098833587514380&ei=7ZamZLeBIcSlmAG0wbfwDg&scisig=ABFrs3yyaKxrp3DiNPUS8C7mf3jj)\n",
        "- [Vahidi, C., Han, H., Wang, C., Lagrange, M., Fazekas, G., & Lostanlen, V. (2023). **Mesostructures: Beyond Spectrogram Loss in Differentiable Time-Frequency Analysis.** arXiv preprint arXiv:2301.10183.](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2301.10183&hl=en&sa=T&oi=gsb&ct=res&cd=0&d=9636633872609712841&ei=-5amZJy0MYekmwHAipuwDA&scisig=ABFrs3x0p-dWnaZd4WVxBEr5jB3j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-nn15M-wUhH"
      },
      "source": [
        "## **Setup**\n",
        "- Run the `installation` cells below\n",
        "- Ensure that imports load and GPU runtime is enabled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIYemXluqf3i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "#@markdown Let's install Kymatio from source and some utilities\n",
        "!pip install git+https://github.com/kymatio/kymatio.git\n",
        "!pip install git+https://github.com/cyrusvahidi/meso-dtfa.git#egg=meso_dtfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAv8gmZHxDER",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import Python libraries\n",
        "\n",
        "#@markdown We will use `Scattering1D` and `TimeFrequencyScattering` PyTorch frontends from Kymatio,\n",
        "#@markdown numpy and some custom signal generators and distances.\n",
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "import tqdm\n",
        "import scipy\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "\n",
        "from kymatio.torch import Scattering1D, TimeFrequencyScattering\n",
        "from kymatio.scattering1d.filter_bank import scattering_filter_factory\n",
        "\n",
        "from meso_dtfa.core import generate_am_chirp, grid2d\n",
        "from meso_dtfa.loss import MultiScaleSpectralLoss, TimeFrequencyScatteringLoss\n",
        "from meso_dtfa.plot import plot_cqt, plot_contour_gradient, mesh_plot_3d\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2DD0qIGxhQf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU Status\n",
        "import subprocess\n",
        "\n",
        "USE_GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "def to_device(tensor: torch.tensor):\n",
        "    \"\"\" Move a torch tensor to the current device\n",
        "    \"\"\"\n",
        "    return tensor.cuda() if USE_GPU else tensor.cpu()\n",
        "\n",
        "#!nvidia-smi\n",
        "nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(nvidiasmi_output)\n",
        "print(f\"Using device: {'GPU' if USE_GPU else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUZke1hIOGxN"
      },
      "source": [
        "## `Scattering1D` (Time Scattering)\n",
        "\n",
        "- $ U_1 x [\\lambda, t] = |x * \\psi_{\\lambda}|(t) $\n",
        "  - constant-Q wavelet scalogram\n",
        "  - convolves the signal $x$ with a constant-Q wavelet filterbak $\\psi$.\n",
        "  - Each filter is described by the geometrically spaced log-frequency variable $\\lambda$\n",
        "- $ S_1 x [\\lambda, t] = (U_1 x * \\phi_T)(\\lambda, t) $\n",
        "  - The **first-order scattering transform** is the result of local averaging $U_1$ with a lowpass filter $\\phi_T$\n",
        "  - It is a a spectral descriptor that offers time-shift invariance upto a support of $T$\n",
        "- $U_2 x [\\lambda, \\lambda_2, t] = |U_1 * \\psi_{\\lambda_2}|(\\lambda, t) $\n",
        "  - High-frequency oscillations in the scalogram are lost in $S_1$ due to averaging\n",
        "  - We recover them by convolving another wavelet filterbank with $U_1$ along time at every frequency bin $\\lambda$\n",
        "- $ S_2 x [\\lambda, \\lambda_2, t] = (U_2 x * \\phi_T)(\\lambda, \\lambda_2, t) $\n",
        "  - **Second-order scattering coefficients**\n",
        "  - to get a locally time-invariant descriptor of temporal modulations, we average $U_2$ with a lowpass filter $\\phi_T$\n",
        "\n",
        "\n",
        "* $t$: time\n",
        "* $\\lambda$: first-order log-frequency variable\n",
        "* $\\lambda_2$: second-order temporal modulation rate variable\n",
        "* $\\psi_{\\lambda}$: first-order wavelet filterbank\n",
        "* $\\psi_{\\lambda_2}$: second-order wavelet filterbank\n",
        "* $\\phi_T$: Gaussian lowpass filter of scale $T$\n",
        "* $U_1$: wavelet scalogram\n",
        "* $S_1$: first-order scattering transform\n",
        "* $S_2$: second-order scattering transform\n",
        "\n",
        "![](https://freight.cargo.site/t/original/i/e44c49dbc48f6fedde0f1e11dfddbe659e0197d518c143db2c23c0843449369a/scattering-diagram.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggvwTFMUOFWc"
      },
      "outputs": [],
      "source": [
        "#@title Construct a `Scattering1D` frontend\n",
        "#@markdown Let's use Kymatio's `torch.Scattering1D` frontend to construct Time Scattering filterbanks\n",
        "\n",
        "\n",
        "#@markdown $J$ defines the maximal wavelet scale and number of octaves. The largest filter of lowest centre frequency is defined over a support of $2^J$.\n",
        "#@markdown At larger $J$, we can cover larger time scales and slower frequencies\n",
        "J = 8 #@param {type:\"slider\", min:1, max:13, step:1}\n",
        "#@markdown $Q$ determines how well we can localize a signal in frequency\n",
        "\n",
        "#@markdown number of filters per octave (first-order):\n",
        "Q1 = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown number of filters per octave (second-order):\n",
        "Q2 = 1 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "#@markdown $T$ controls the amount of imposed invariance to time-shifts. This defines the scale and stride of the lowpass filters, which is set to $2^J$ by default.\n",
        "T = 256 #@param {type:\"slider\", min:1, max:8192, step:1}\n",
        "Q = (Q1, Q2)\n",
        "\n",
        "#@markdown We must specify the signal length $N$, which defines the temporal support of the filters\n",
        "log2_n = 13 #@param {type:\"slider\", min:4, max:16, step:1}\n",
        "N = 2**log2_n\n",
        "x = torch.randn((N, ))\n",
        "\n",
        "scat1d = to_device(Scattering1D(shape=x.shape, J=J, Q=Q, T=T))\n",
        "\n",
        "phi_f = scat1d.phi_f\n",
        "psi1_f = scat1d.psi1_f\n",
        "psi2_f = scat1d.psi2_f\n",
        "N_padded = scat1d._N_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoVcGhVTQyfQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Plotting the wavelet filterbanks\n",
        "#@markdown Let's plot the frequency response of first and second-order filterbanks `psi1_f` and `psi2_f`\n",
        "\n",
        "#@markdown Try changing $J$, $Q_1$ and $Q_2$. What do you observe?\n",
        "\n",
        "def plot_filterbank(phi, psi, N, order=1):\n",
        "    _ = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    _ = plt.plot(np.arange(N_padded) / (N_padded), phi['levels'][0], 'r')\n",
        "\n",
        "    for psi_f in psi:\n",
        "        plt.plot(np.arange(N_padded)/(N_padded), psi_f['levels'][0], 'b')\n",
        "\n",
        "    plt.xlabel(r'$\\omega$', fontsize=18)\n",
        "    plt.ylabel(r'$\\hat\\psi_j(\\omega)$', fontsize=18)\n",
        "    _ = plt.title(f'order {order} filters', fontsize=18)\n",
        "\n",
        "plot_filterbank(phi_f, psi1_f, N_padded, order=1)\n",
        "plot_filterbank(phi_f, psi2_f, N_padded, order=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLkXJX3V4gy"
      },
      "source": [
        "## Filterbank details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EmRAbcHWBxD"
      },
      "source": [
        "We create the filters by constructing an instance of the `kymatio.torch.Scattering1D` frontend.\n",
        "\n",
        "- `phi_f` contains lowpass filters at various resolutions, For example, `phi_f[0]` is at resolution `T`.\n",
        "- `psi1_f` contains the first-order Morlet wavelet filters\n",
        "- `psi2_f` contains the second-order Morlet wavelet filters\n",
        "- `psi1_f` and `psi2_f` differ in choice of `Q`\n",
        "\n",
        "\n",
        "Each filter is structured as a dictionary containing:\n",
        "- the filter coefficients (in Fourier)\n",
        "- `xi`: the filter's normalized centre frequency\n",
        "- `sigma`: the filter's width\n",
        "- `j`: the filter's scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8aiCEIZPyd9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Let's take a look at a filters' metadata\n",
        "\n",
        "print(f\"Temporal support of the lowpass filter at each resolution: {[len(phi) for phi in phi_f['levels']]}\")\n",
        "\n",
        "print(f\"First filter coefficient: {psi1_f[0]['levels'][0]}\")\n",
        "\n",
        "print(f\"First filter scale, j: {psi1_f[0]['j']}\")\n",
        "\n",
        "print(f\"A first order filter's central frequency $xi$ in Hz, assuming a sampling rate of 4096 Hz:  {psi1_f[0]['xi'] * 4096}\")\n",
        "\n",
        "print(f\"A first order filter's characteristic width, sigma: {psi1_f[0]['sigma']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2kOwKFoXeLq"
      },
      "outputs": [],
      "source": [
        "#@title Plotting the first-order coefficients, $S_1$\n",
        "#@markdown This is like a time-averaged CQT\n",
        "import librosa\n",
        "\n",
        "def plot_scalogram(Sx, S, duration, sr=22050):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      Sx: first-order scattering coefficients\n",
        "      S: scattering transform frontend\n",
        "    \"\"\"\n",
        "    x_coords = librosa.times_like(Sx, hop_length=S.T)\n",
        "    y_coords = [psi[\"xi\"]*sr for psi in S.psi1_f] # convert xi to Hz\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    librosa.display.specshow(Sx[1:,:].numpy(), sr=sr,\n",
        "        x_coords=x_coords, x_axis=\"time\",\n",
        "        y_coords=y_coords, y_axis=\"cqt_hz\",\n",
        "        cmap=\"magma\")\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Frequency (Hz)\")\n",
        "    plt.ylim(0, sr // 2)\n",
        "    plt.minorticks_off()\n",
        "\n",
        "N = 2**15\n",
        "SR = 2**15\n",
        "kwargs = {'shape': N, 'Q': Q, 'J': J, 'T': T}\n",
        "\n",
        "#@markdown we set `max_order=1` to only compute first-order coefficients\n",
        "S = to_device(Scattering1D(**kwargs, max_order=1))\n",
        "\n",
        "#@markdown Let's plot the $S_1$ of an impulse\n",
        "x = to_device(torch.zeros(N))\n",
        "x[N // 2] = 1\n",
        "Sx = S(x)\n",
        "plot_scalogram(Sx.cpu(), S, duration=1, sr=SR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting $S_1$ of an AM/FM signal\n",
        "\n",
        "#@markdown Next, an amplitude-modulated chirp signal, or chirplet arpeggiator\n",
        "\n",
        "#@markdown $    \\boldsymbol{g_\\theta}: t\\longmapsto \\boldsymbol{\\phi}_{w}(\\gamma t) \\sin(2\\pi f_\\mathrm{m} t) \\sin\\left( \\dfrac{2\\pi f_{\\mathrm{c}}}{\\gamma \\log 2} 2^{\\gamma t}\\right)$\n",
        "\n",
        "#@markdown amplitude modulation (AM) is parameterised by modulator frequency $f_m$ and frequency modulation (FM) by chirp rate $\\gamma$\n",
        "\n",
        "#@markdown The chirp's instantaneous frequency increases exponentially with time, while it's amplitude is modulated by a sinusoid.\n",
        "\n",
        "#@markdown Perceived pitch therefore grows linearly, and we see a linear change along our log-frequency axis\n",
        "\n",
        "#@markdown see `help(generate_am_chirp)` for details on the synth\n",
        "fc, fm, gamma = torch.tensor(512.0), torch.tensor(4.0), torch.tensor(1.0)\n",
        "x = generate_am_chirp([fc, fm, gamma], sr=SR, duration = 1)\n",
        "Sx = S(to_device(x))\n",
        "plot_scalogram(Sx.cpu(), S, duration=1, sr=SR)\n",
        "\n",
        "#@markdown What happens to the amplitude modulations in $S_1$ if you increase T or $f_m$?"
      ],
      "metadata": {
        "id": "krSMdOeG1cfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwssVWhQX6Iv"
      },
      "outputs": [],
      "source": [
        "help(generate_am_chirp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c3TNn5AEHQ"
      },
      "source": [
        "## Getting first and second order coefficients\n",
        "* `Scattering1D` returns a vector per timestep, concatenating zeroth, first and second order coefficients of shape `(num_paths, timesteps)`\n",
        "* To display the scattering coefficients, we must identify the indices for each order.\n",
        "* A `Scattering1D` object contains metadata that includes the indices for each scattering order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP9zfR4MAEHQ"
      },
      "outputs": [],
      "source": [
        "Fs = 2**14 # sampling rate\n",
        "duration = 2 # signal duration\n",
        "J = 12 # maximum wavelet scattering scale\n",
        "Q = 8\n",
        "\n",
        "scat1d = Scattering1D(J=J, Q=Q, shape=duration * Fs, T=2**J)\n",
        "\n",
        "meta = scat1d.meta()\n",
        "order0 = np.where(meta['order'] == 0)[0] # zeroth-order indices\n",
        "order1 = np.where(meta['order'] == 1)[0] # first-order indices\n",
        "order2 = np.where(meta['order'] == 2)[0] # second-order indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqfTvOoaAEHQ"
      },
      "source": [
        "## Visualizing $U_1$, $S_1$ and $S2$ of an AM signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mokHuqAAEHR"
      },
      "outputs": [],
      "source": [
        "#@markdown Let's create a harmonic tremolo signal.\n",
        "\n",
        "#@markdown To help with visualisation, we set its fundamental frequency to the centre frequency of a first-order wavelet filter and the modulation frequency to the centre frequency of a second-order wavelet filter.\n",
        "\n",
        "f0_xi_idx = 30 # first-order filter index for the fundamental frequency\n",
        "fm_xi_idx = 8 # second-order filter index for the modulationf requency\n",
        "\n",
        "f0 = meta['xi'][order1[f0_xi_idx], 0] * Fs # get the fundamental frequency\n",
        "fm = meta['xi'][order2[fm_xi_idx], 1] * Fs # get the modulation frequency\n",
        "print(f'fundamental frequency: {f0} Hz')\n",
        "print(f'modulation frequency: {fm} Hz')\n",
        "\n",
        "#@markdown We synthesize a harmonic signal with 5 harmonics that are amplitude-modulated by a sinusoid\n",
        "num_harmonics = 5\n",
        "harmonics = torch.zeros(num_harmonics, duration * Fs)\n",
        "t = torch.arange(0, duration, float(1/Fs))\n",
        "modulator = torch.sin(2.0 * np.pi * fm * t)\n",
        "harmonics[0] = torch.sin(2.0 * np.pi * f0 * t) * modulator\n",
        "\n",
        "for i in range(1, num_harmonics):\n",
        "    harmonics[i] = torch.cos(2.0 * np.pi * (f0 * i) * t) * modulator\n",
        "\n",
        "x_am = torch.sum(harmonics, dim=0) # sum the harmonics\n",
        "x_am *= torch.hann_window(x_am.shape[0]).numpy()\n",
        "x_am /= torch.max(torch.abs(x_am)) # normalize the amplitude\n",
        "\n",
        "Audio(x_am, rate=Fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkO0T9oXAEHS"
      },
      "outputs": [],
      "source": [
        "#@title $U_1$ vs $S_1$ vs $S_2$\n",
        "def compute_scattering(x, scat1d_u, scat1d, lambda1_idx=None):\n",
        "    \"\"\"Compute U1, S1, and S2 for the given signal.\n",
        "\n",
        "    Parameters:\n",
        "        x (ndarray): Input signal.\n",
        "        scat1d_u (Scattering1D): Scattering1D instance with average=False.\n",
        "        scat1d (Scattering1D): Scattering1D instance.\n",
        "        lambda1_idx (int or None): Target lambda1 index (first-order parent) for S2 visualization.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the following elements:\n",
        "            u1 (Tensor): Unaveraged scalogram of x.\n",
        "            s1 (ndarray): First-order scattering transform of x.\n",
        "            s2 (ndarray): Second-order scattering transform of x.\n",
        "    \"\"\"\n",
        "    # compute the unaveraged scattering transform Ux\n",
        "    Ux = scat1d_u(x)\n",
        "    # get the first-order coefficients\n",
        "    Ux = [u['coef'] for u in Ux if u['order'] == 1]\n",
        "    max_resolution = max([u.shape[-1] for u in Ux]) # largest length of unaveraged transform\n",
        "    # resample the first order coefficient to same temporal shape\n",
        "    Ux = [scipy.signal.resample(u.numpy(), max_resolution, axis=-1) for u in Ux]\n",
        "    Ux = torch.tensor(np.stack(Ux))\n",
        "\n",
        "    meta = scat1d.meta()\n",
        "    order1 = np.where(meta['order'] == 1)[0]\n",
        "\n",
        "    ''' Select indices of order2 coefficients that corresponds to the\n",
        "        order1 parent lambda1_idx\n",
        "    '''\n",
        "    if lambda1_idx:\n",
        "        # get s2 keys\n",
        "\n",
        "        order2 = [i for i, x in enumerate(scat1d.meta()['key']) if x and x[0] == lambda1_idx and scat1d.meta()['order'][i] == 2]\n",
        "    else:\n",
        "        order2 = np.where(meta['order'] == 2)[0]\n",
        "\n",
        "    '''Compute the scattering transform'''\n",
        "    Sx = scat1d(x)\n",
        "    S1 = Sx[order1]\n",
        "    S2 = Sx[order2]\n",
        "\n",
        "    return Ux, S1, S2\n",
        "\n",
        "scat1d = Scattering1D(J=10, Q=12, shape=duration * Fs) # scattering transform\n",
        "scat1d_u = Scattering1D(J=10, Q=12, shape=duration * Fs, T=0, out_type=\"list\") # scattering transform without averaged coefficients\n",
        "\n",
        "U1, S1, S2 = compute_scattering(x_am, scat1d_u, scat1d, lambda1_idx=f0_xi_idx)\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize=(8, 15))\n",
        "for i, (title, sx) in enumerate([(\"$U_1$\", U1), (\"$S_1$\", S1), (\"$S_2$\", S2)]):\n",
        "  ax[i].set_xticks([])\n",
        "  ax[i].set_yticks([])\n",
        "  ax[i].set_title(title)\n",
        "  # plt.ylabel(\"$\\lambda$ ($\\log$-frequency)\")\n",
        "  # plt.xlabel(\"time\")\n",
        "  ax[i].imshow(sx, aspect=\"auto\")\n",
        "fig.tight_layout()\n",
        "\n",
        "#@markdown Observe the difference in temporal resolution between $U_1$ and $S_1$/$S_2$\n",
        "#@markdown Why have the amplitude modulation disappeared in $S_1$?\n",
        "#@markdown We only plot $S_2$ around a particular first-order parent, i.e. the $\\lambda_1$ that is centred around the $f_0$ of the signal\n",
        "#@markdown Since our modulations are \"slow\", we see a response in the lowest octave of $S_2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6C3vV2JMskX"
      },
      "source": [
        "# `TimeFrequencyScattering` (JTFS)\n",
        "\n",
        "Let's take a look at `TimeFrequencyScattering`. Unlike `Scattering1D`, JTFS analyses the time-frequency domain ($U_1$) with a cascade of two filterbanks defined along time and frequency: $\\psi_{\\alpha}^{(t)}$ and $\\psi_{\\beta}^{(f)}$\n",
        "\n",
        "This results in a 2-dimensional filterbank, that analyses joint spectrotemporal modulation patterns\n",
        "\n",
        "- $ U_1 x [\\lambda, t] = |x * \\psi_{\\lambda}| $ constant-Q wavelet scalogram\n",
        "- $ S_1 x [\\lambda, \\alpha = 0, \\beta t] = |U_1 x(\\lambda, t) * \\phi_T * \\psi_{\\beta}| $ first-order time-frequency scattering transform\n",
        "- $ U_2 x [\\lambda, \\alpha, \\beta, t] = |U_1 x(\\lambda, t) * \\psi_{\\alpha} * \\psi_{\\beta}| $\n",
        "- $ S_2 x [\\lambda, \\lambda_2, t] = (U_2 x(\\lambda, \\alpha, \\beta, t) * \\phi_T * \\phi_F) $ second-order time-frequency scattering transform\n",
        "\n",
        "\n",
        "* $t$: time\n",
        "* $\\lambda$: first-order log-frequency\n",
        "* $\\alpha$: second-order temporal modulation rate\n",
        "* $\\beta$: frequential modulation scale - positive and negative\n",
        "* $\\psi_{\\lambda}^{(t)}$: first-order wavelet filterbank\n",
        "* $\\psi_{\\alpha}^{(t)}$: second-order temporal wavelet filterbank\n",
        "* $\\psi_{\\beta}^{(f)}$: frequential wavelet filterbank\n",
        "* $\\phi_T$: Gaussian lowpass filter of scale $T$\n",
        "* $\\phi_F$: Gaussian frequential lowpass filter of scale $F$\n",
        "* $U_1$: wavelet scalogram\n",
        "* $S_1$: first-order time-frequency scattering transform\n",
        "* $S_2$: second-order time-frequency scattering transform\n",
        "\n",
        "\n",
        "The image below illustrates the 2-dimensional shapes of the filters (left) and their response to a signal containing a sinusoid, impulse and a chirp:\n",
        "![](https://freight.cargo.site/t/original/i/88e0cdfbc9fd77878caa7a628d7dbf337aad5c1608ec0a45e672b2450b42e44a/Screenshot-2023-07-07-at-14.15.19.png)\n",
        "Each filter varies in temporal modulation rate, scale in frequency and orientation along the frequency axis.\n",
        "The more compact filters focus on fast oscillations. Vertically oriented filters ignore frequential modulations, while horizontally oriented filters ignore amplitude modulations.\n",
        "A sinusoid is like an impulse on the frequency axis, so we see a response across the entire frequential filterbank's spectrum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjyReT5rMoO3"
      },
      "source": [
        "## Manifold Embedding of Spectrotemporal Modulations\n",
        "Let's return to the AM/FM chirp signal. We will compare how various acoustic features represent similarity between sound generated by the synth as its 3 parameters vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yvQgLHCMrXh"
      },
      "outputs": [],
      "source": [
        "SR = 2**13\n",
        "\n",
        "fc = 362 #@param {type:\"slider\", min:128.0, max:1024.0, step:1.0}\n",
        "am = 15 #@param {type:\"slider\", min:4.0, max:16.0, step:1.0}\n",
        "fm = 1.5 #@param {type:\"slider\", min:0.5, max:4.0, step:0.5}\n",
        "\n",
        "fc = torch.tensor(fc, dtype=torch.float32)\n",
        "am = torch.tensor(am, dtype=torch.float32)\n",
        "fm = torch.tensor(fm, dtype=torch.float32)\n",
        "x = generate_am_chirp([fc, am, fm], bw=2, duration=4)\n",
        "\n",
        "\n",
        "plot_cqt(x.numpy(), sr=SR)\n",
        "Audio(x, rate=SR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFEardwiAEHU"
      },
      "outputs": [],
      "source": [
        "#@title Similarity Retrieval\n",
        "import os, tqdm\n",
        "from sklearn.manifold import Isomap\n",
        "import numpy as np, matplotlib.pyplot as plt, scipy\n",
        "\n",
        "def run_isomap(X, params, n_neighbors=40):\n",
        "    model = Isomap(n_components=3, n_neighbors=n_neighbors)\n",
        "    Y = model.fit_transform(X)\n",
        "\n",
        "    plot_isomap(Y, params)\n",
        "\n",
        "\n",
        "def plot_isomap(Y, params):\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
        "    axs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        ax = fig.add_subplot(1, 3, i + 1, projection='3d')\n",
        "        ax.scatter3D(Y[:, 0], Y[:, 1], Y[:, 2], c=params[i], cmap='bwr');\n",
        "\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_zticklabels([])\n",
        "        axs.append(ax)\n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "    # rotate the axes and update\n",
        "    for angle in range(60, 360, 60):\n",
        "        for ax in axs:\n",
        "            ax.view_init(30, angle)\n",
        "            plt.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwrMd-W1AEHU"
      },
      "outputs": [],
      "source": [
        "#@title Synthetic AM chirp dataset generation\n",
        "#@markdown We generate 4096 signals lying on a 3D manifold in ($f_c$, $f_m$, $\\gamma$)\n",
        "\n",
        "#@markdown $f_c \\in [512, 1024]$\n",
        "\n",
        "#@markdown $f_m \\in [4, 16]$\n",
        "\n",
        "#@markdown $\\gamma \\in [0.5, 4]$\n",
        "n_steps = 16\n",
        "f0_min, f0_max = 512, 1024\n",
        "am_min, am_max = 4, 16\n",
        "fm_min, fm_max = 0.5, 4\n",
        "bw = 2\n",
        "duration = 4\n",
        "sr = 2**13\n",
        "f0s = np.logspace(np.log10(f0_min), np.log10(f0_max), n_steps)\n",
        "AM = np.logspace(np.log10(am_min), np.log10(am_max), n_steps)\n",
        "FM = np.logspace(np.log10(fm_min), np.log10(fm_max), n_steps)\n",
        "\n",
        "audio = np.zeros((len(f0s), len(AM), len(FM), duration * sr))\n",
        "params = np.zeros((3, len(f0s) * len(AM) * len(FM)))\n",
        "c = 0\n",
        "\n",
        "print('Generating Audio ...')\n",
        "for i, f0 in tqdm.tqdm(enumerate(f0s)):\n",
        "    for j, am in enumerate(AM):\n",
        "        for k, fm in enumerate(FM):\n",
        "            theta = torch.tensor([f0, am, fm])\n",
        "            x = generate_am_chirp(theta, sr=sr, duration=duration).numpy()\n",
        "            audio[i, j, k, :] = x / np.linalg.norm(x)\n",
        "            params[0, c], params[1, c], params[2, c] = f0, am, fm\n",
        "            c += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDdEaWRtAEHV"
      },
      "outputs": [],
      "source": [
        "#@title Similarity Retrieval - `MFCC`\n",
        "n_mfcc = 40\n",
        "\n",
        "Sx = np.zeros((len(f0s), len(AM), len(FM), n_mfcc))\n",
        "\n",
        "print('Extracting MFCCs ...')\n",
        "for i, f0 in tqdm.tqdm(enumerate(f0s)):\n",
        "    for j, fm in enumerate(AM):\n",
        "        for k, gamma in enumerate(FM):\n",
        "            Sx[i, j, k,:] = np.mean(librosa.feature.mfcc(y=audio[i,j,k], sr=sr, n_mfcc=n_mfcc), axis=-1)\n",
        "mfccs = Sx.reshape(-1, Sx.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_isomap(mfccs, params)"
      ],
      "metadata": {
        "id": "4k1v6iAmgUSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKbA7m3mAEHV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "#@title Similarity Retrieval - `Scattering1D`\n",
        "batch_size = 128\n",
        "N = duration * sr\n",
        "Q = 1 #@param {type:\"slider\", min:1, max:12, step:1}\n",
        "scat = Scattering1D(shape=(N, ), T=N, Q=Q, J=int(np.log2(N) - 1))\n",
        "scat = scat.cuda()\n",
        "\n",
        "X = torch.tensor(audio.reshape(-1, audio.shape[-1]), dtype=torch.float32).cuda()\n",
        "n_samples = X.shape[0]\n",
        "n_paths = scat(X[0]).shape[0]\n",
        "\n",
        "Sx = torch.zeros(n_samples, n_paths).cuda()\n",
        "\n",
        "for i in tqdm.tqdm(range(math.ceil(n_samples / batch_size))):\n",
        "    start = i * batch_size\n",
        "    end = (i + 1) * batch_size\n",
        "    Sx[start:end, :] = scat(X[start:end, :])[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_isomap(Sx.cpu().numpy(), params)"
      ],
      "metadata": {
        "id": "1jNpNdlHghAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKArlPwMAEHW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Similarity Retrieval - `TimeFrequencyScattering`\n",
        "import math\n",
        "batch_size = 16\n",
        "N = duration * sr\n",
        "jtfs = TimeFrequencyScattering(shape=(N,),\n",
        "                               T=N,\n",
        "                               Q=(8, 1),\n",
        "                               J=13, # int(np.log2(N) - 1),\n",
        "                               Q_fr=2,\n",
        "                               J_fr=5,\n",
        "                               F=0,\n",
        "                               format=\"time\")\n",
        "jtfs = jtfs.cuda()\n",
        "\n",
        "Q = 8 #@param {type:\"slider\", min:1, max:12, step:1}\n",
        "\n",
        "X = torch.tensor(audio.reshape(-1, audio.shape[-1]),\n",
        "                 dtype=torch.float32)\n",
        "n_samples, n_paths = X.shape[0], jtfs(X[0].cuda()).shape[0]\n",
        "\n",
        "Sx = torch.zeros(n_samples, n_paths)\n",
        "\n",
        "for i in tqdm.tqdm(range(math.ceil(n_samples / batch_size))):\n",
        "    start = i * batch_size\n",
        "    end = (i + 1) * batch_size\n",
        "    Sx[start:end, :] = jtfs(X[start:end, :].cuda())[:, :, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_isomap(Sx, params)"
      ],
      "metadata": {
        "id": "0h1g6fVqpZYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differentiable Parametric Similarity Retrieval"
      ],
      "metadata": {
        "id": "jK_X4Ch9mQDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojsnEyS2AEHW"
      },
      "outputs": [],
      "source": [
        "#@markdown Back to our arpeggiator ...\n",
        "#@markdown this thing is a differentiable ... $g(\\theta)$\n",
        "#@markdown we can compose it with another function $\\Phi = S \\cdot g(\\theta)$\n",
        "#@markdown then get gradient of this operator with respect to $\\theta$\n",
        "#@markdown if $S$ represents $\\theta$ well, we can backpropagate through $\\Phi$ to model $\\theta$ and solve optimisation problems with our artificial hearing device\n",
        "#@markdown let's call this a differentiable mesostructural operator\n",
        "\n",
        "def run_gradient_viz(loss_type=\"jtfs\", time_shift=None):\n",
        "    f0 = torch.tensor([512.0], dtype=torch.float32, requires_grad=False).cuda()\n",
        "    N = 20\n",
        "\n",
        "    target_idx = N * (N // 2) + (N // 2)\n",
        "\n",
        "    AM, FM = grid2d(x1=4, x2=16, y1=0.5, y2=4, n=N)\n",
        "    X = AM.numpy().reshape((N, N))\n",
        "    Y = FM.numpy().reshape((N, N))\n",
        "    AM.requires_grad = True\n",
        "    FM.requires_grad = True\n",
        "    thetas = torch.stack([AM, FM], dim=-1).cuda()\n",
        "\n",
        "    sr = 2**13\n",
        "    duration = 4\n",
        "    n_input = sr * duration\n",
        "\n",
        "    theta_target = thetas[target_idx].clone().detach().requires_grad_(False)\n",
        "    target = (\n",
        "        generate_am_chirp(\n",
        "            [f0, theta_target[0], theta_target[1]], sr=sr, duration=duration\n",
        "        )\n",
        "        .cuda()\n",
        "        .detach()\n",
        "    )\n",
        "\n",
        "    if loss_type == \"jtfs\":\n",
        "        loss_fn = TimeFrequencyScatteringLoss(\n",
        "            shape=(n_input,),\n",
        "            #T=2**13,\n",
        "            Q=(8, 2),\n",
        "            J=12,\n",
        "            J_fr=5,\n",
        "            F=\"global\",\n",
        "            Q_fr=2,\n",
        "            format=\"time\",\n",
        "        )\n",
        "        Sx_target = loss_fn.ops[0](target.cuda()).detach()\n",
        "    elif loss_type == \"mss\":\n",
        "        loss_fn = MultiScaleSpectralLoss(max_n_fft=1024)\n",
        "\n",
        "    x, y, u, v = [], [], [], []\n",
        "    losses, grads = [], []\n",
        "    for theta in tqdm(thetas):\n",
        "        am = torch.tensor(theta[0], requires_grad=True, dtype=torch.float32)\n",
        "        fm = torch.tensor(theta[1], requires_grad=True, dtype=torch.float32)\n",
        "        audio = generate_am_chirp(\n",
        "            [torch.tensor([768.0], dtype=torch.float32, requires_grad=False).cuda(), am, fm],\n",
        "            sr=sr,\n",
        "            duration=duration,\n",
        "            delta=(2 ** random.randint(8, 12) if time_shift == \"random\" else 2**8)\n",
        "            if time_shift\n",
        "            else 0,\n",
        "        )\n",
        "\n",
        "        loss = (\n",
        "            loss_fn(audio.cuda(), Sx_target.cuda(), transform_y=False)\n",
        "            if loss_type == \"jtfs\"\n",
        "            else loss_fn(audio, target)\n",
        "        )\n",
        "        loss.backward()\n",
        "        losses.append(float(loss.detach().cpu().numpy()))\n",
        "        x.append(float(am))\n",
        "        y.append(float(fm))\n",
        "        u.append(float(-am.grad))\n",
        "        v.append(float(-fm.grad))\n",
        "\n",
        "        grad = np.stack([float(-am.grad), float(-fm.grad)])\n",
        "        grads.append(grad)\n",
        "\n",
        "    zs = np.array(losses)\n",
        "    Z = zs.reshape(X.shape)\n",
        "\n",
        "    plot_contour_gradient(\n",
        "        X,\n",
        "        Y,\n",
        "        Z,\n",
        "        target_idx,\n",
        "        grads,\n",
        "    )\n",
        "    mesh_plot_3d(\n",
        "        X, Y, Z, target_idx,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "vUTCZn5QwueX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}